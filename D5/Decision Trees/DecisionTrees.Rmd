---
title: "Decision Trees iFood"
author: "iFood Marketing Analytics"
date: "`r format(Sys.Date(), '%d %B %Y')`"
output:
  html_document:
  toc: true
toc_float: true
toc_depth: 3
theme: united
highlight: tango
code_folding: show
pdf_document: default
---
  
# Global knitr configuration for the R Markdown document:
```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = TRUE,
  fig.width = 10,
  fig.height = 7
)
```

# ==========================================
# 0. Needed libraries
# ==========================================
```{r}
library(readr)
library(dplyr)
library(caret)
library(rpart)
library(rpart.plot)
library(pROC)          
library(randomForest)
```

# ==========================================
# 1. Load and preparation
# ==========================================
```{r}
data <- read_csv("ifood_enriched.csv", show_col_types = FALSE)
names(data) <- make.names(names(data))

# Basic Imputation
data$Income[is.na(data$Income)] <- median(data$Income, na.rm = TRUE)

# Variable Selection
df_model <- data %>%
  select(
    Response, Income, Age, Recency,
    WineExp, MeatExp, FishExp, SweetExp, FruitExp, GoldExp, TotalExp,
    TotalPurchases, DealsPurc, WebPurc, CatalogPurc, StorePurc,
    Kidhome, Teenhome, Education, MaritalSts, 
    WebVisits, AccCmp3, AccCmp4, AccCmp5, AccCmp1, AccCmp2, Complain,
    CustomerTenure, CampaignAcceptanceRate, PropensityScore, EngagementIndex
  ) %>%
  na.omit()

# Caret works better with text labels for classes rather than "0" and "1"
df_model$Response <- factor(df_model$Response, 
                            levels = c(0, 1), 
                            labels = c("No", "Yes"))
```

# ==========================================
# 2. Data partitioning
# ==========================================
```{r}
set.seed(42)
index <- createDataPartition(df_model$Response, p = 0.7, list = FALSE)
train_data <- df_model[index, ]
test_data  <- df_model[-index, ]
```

# ==========================================
# 3. Advanced Training 
# ==========================================
# Training control configuration
# sampling = "up" -> Makes a "yes" more important
```{r}
ctrl <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,                # Required to calculate probabilities
  summaryFunction = twoClassSummary,# Optimization for ROC instead of Accuracy
  sampling = "up",                  
  verboseIter = FALSE
)
```

```{r}
# Train the Random Forest
set.seed(42)
rf_model <- train(
  Response ~ .,
  data = train_data,
  method = "rf",
  metric = "ROC",     # Aiming for the best balance between detection and precision
  trControl = ctrl,
  tuneLength = 5      # Automatically test different configurations
)
```

# ==========================================
# 4. Threshold Assessment and Adjustment
# ==========================================
```{r}
# Predict Probabilities instead of direct classes
probs <- predict(rf_model, test_data, type = "prob")

# Calculate ROC curve
roc_obj <- roc(test_data$Response, probs$Yes, levels = c("No", "Yes"))

# Find the best cutoff point (Threshold)
# This balances detecting buyers vs. avoiding disturbing non-buyers
best_coords <- coords(roc_obj, "best", best.method = "closest.topleft", ret = "threshold")
best_threshold <- best_coords$threshold

# Apply the custom threshold
preds_class <- factor(ifelse(probs$Yes > best_threshold, "Yes", "No"), levels = c("No", "Yes"))

# Final Confusion Matrix
cm_rf <- confusionMatrix(preds_class, test_data$Response, positive = "Yes")
```

# ==========================================
# 5. Results and variable importance
# ==========================================
```{r}
print(cm_rf)
cat("\nAUC (Area Under the Curve):", auc(roc_obj))
cat("\nBest Decision Threshold:", best_threshold)

# Variable Importance Plot
# This indicates which variables actually drive the decision
varImpPlot(rf_model$finalModel, main = "Top Influential Variables for Purchase", n.var = 20)
```

# ==========================================
# 6. Representative tree visualization
# ==========================================
```{r}
# Train a simple tree JUST for visualization of business rules.
# We mimic the balancing effect performed by the Random Forest.

tree_viz <- rpart(
  Response ~ ., 
  data = train_data,
  method = "class",
  parms = list(prior = c(0.5, 0.5)),  # Trick to visually balance importance
  control = rpart.control(cp = 0.005, maxdepth = 5) # Limit depth for readability
)

# Print the tree
rpart.plot(tree_viz,
           main = "Business Rules Tree (Buyer Profile)",
           type = 4,             # Draws nice labels
           extra = 104,          # Shows probability for each class (multiclass)
           under = TRUE,         # Places the % under the box
           faclen = 0,           # Shows full variable names
           cex = 0.7,            # Text size
           box.palette = "BuGn"  # Colors: Green for YES, Blue/Grey for NO
)
```

# ==========================================
# 7. Decision Boundary Heatmap
# ==========================================
```{r, fig.width=10, fig.height=8}
library(ggplot2)
library(viridis)

# 1. Identify variables
var1 <- "EngagementIndex"
var2 <- "PropensityScore"

# 2. Create Grid
grid_size <- 100
x_range <- seq(min(train_data[[var1]]), max(train_data[[var1]]), length.out = grid_size)
y_range <- seq(min(train_data[[var2]]), max(train_data[[var2]]), length.out = grid_size)
grid_data <- expand.grid(x = x_range, y = y_range)
names(grid_data) <- c(var1, var2)

# 3. Fill other columns (ROBUST METHOD)
# We fill missing columns so the model can run. 
# We use median for numbers and the first valid value for text/factors.
other_cols <- setdiff(names(train_data), c(var1, var2, "Response"))

for(col in other_cols){
  # Check if the column exists in train_data to avoid errors
  if(col %in% names(train_data)){
    if(is.numeric(train_data[[col]])){
      # If numeric, fill with Median
      grid_data[[col]] <- median(train_data[[col]], na.rm=TRUE)
    } else {
      # If text or factor, fill with the first valid unique value found
      # This fixes the "Education not found" error by handling characters correctly
      first_val <- na.omit(unique(train_data[[col]]))[1]
      grid_data[[col]] <- first_val
    }
  }
}

# 4. PREDICTION
# We calculate probabilities using the Random Forest
probs_grid <- predict(rf_model, newdata = grid_data, type = "prob")

# Robustly select the "Yes" column (usually the 2nd one)
grid_data$Probability <- probs_grid[, 2]

# 5. PLOT
ggplot(grid_data, aes_string(x = var1, y = var2, fill = "Probability")) +
  geom_tile() + 
  scale_fill_viridis(option = "plasma", direction = -1, name = "Prob. of Yes") +
  
  # Overlay REAL points from test data
  geom_point(data = test_data, aes_string(x = var1, y = var2, color = "Response"), 
             alpha = 0.6, size = 1.5, inherit.aes = FALSE) +
  scale_color_manual(values = c("black", "cyan"), 
                     labels = c("Did Not Buy", "Bought"), 
                     name = "Actual Outcome") +
  
  theme_minimal(base_size = 14) +
  labs(
    title = "Decision Boundary:",
    subtitle = "Visualizing the probability of saying 'Yes' based on the top 2 drivers",
    x = var1,
    y = var2
  ) +
  theme(legend.position = "right")
```
