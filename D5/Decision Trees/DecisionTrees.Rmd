---
title: "Decision Trees iFood"
author: "iFood Marketing Analytics"
date: "`r format(Sys.Date(), '%d %B %Y')`"
output:
  html_document:
  toc: true
toc_float: true
toc_depth: 3
theme: united
highlight: tango
code_folding: show
pdf_document: default
---
  
# Global knitr configuration for the R Markdown document:
```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = TRUE,
  fig.width = 10,
  fig.height = 7
)
```

# ==========================================
# 0. Needed libraries
# ==========================================
```{r}
library(readr)
library(dplyr)
library(caret)
library(rpart)
library(rpart.plot)
library(pROC)          
library(randomForest)
```

# ==========================================
# 1. Load and preparation
# ==========================================
```{r}
data <- read_csv("ifood_enriched.csv", show_col_types = FALSE)
names(data) <- make.names(names(data))

# Basic Imputation
data$Income[is.na(data$Income)] <- median(data$Income, na.rm = TRUE)

# Variable Selection
df_model <- data %>%
  select(
    Response, Income, Age, Recency,
    WineExp, MeatExp, FishExp, SweetExp, FruitExp, GoldExp, TotalExp,
    TotalPurchases, DealsPurc, WebPurc, CatalogPurc, StorePurc,
    Kidhome, Teenhome, Education, MaritalSts, 
    WebVisits, AccCmp3, AccCmp4, AccCmp5, AccCmp1, AccCmp2, Complain,
    CustomerTenure, CampaignAcceptanceRate, PropensityScore, EngagementIndex
  ) %>%
  na.omit()

# Caret works better with text labels for classes rather than "0" and "1"
df_model$Response <- factor(df_model$Response, 
                            levels = c(0, 1), 
                            labels = c("No", "Yes"))
```

# ==========================================
# 2. Data partitioning
# ==========================================
```{r}
set.seed(42)
index <- createDataPartition(df_model$Response, p = 0.7, list = FALSE)
train_data <- df_model[index, ]
test_data  <- df_model[-index, ]
```

# ==========================================
# 3. Advanced Training 
# ==========================================
# Training control configuration
# sampling = "up" -> Makes a "yes" more important
```{r}
ctrl <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,                # Required to calculate probabilities
  summaryFunction = twoClassSummary,# Optimization for ROC instead of Accuracy
  sampling = "up",                  
  verboseIter = FALSE
)
```

```{r}
# Train the Random Forest
set.seed(42)
rf_model <- train(
  Response ~ .,
  data = train_data,
  method = "rf",
  metric = "ROC",     # Aiming for the best balance between detection and precision
  trControl = ctrl,
  tuneLength = 5      # Automatically test different configurations
)
```

# ==========================================
# 4. Threshold Assessment and Adjustment
# ==========================================
```{r}
# Predict Probabilities instead of direct classes
probs <- predict(rf_model, test_data, type = "prob")

# Calculate ROC curve
roc_obj <- roc(test_data$Response, probs$Yes, levels = c("No", "Yes"))

# Find the best cutoff point (Threshold)
# This balances detecting buyers vs. avoiding disturbing non-buyers
best_coords <- coords(roc_obj, "best", best.method = "closest.topleft", ret = "threshold")
best_threshold <- best_coords$threshold

# Apply the custom threshold
preds_class <- factor(ifelse(probs$Yes > best_threshold, "Yes", "No"), levels = c("No", "Yes"))

# Final Confusion Matrix
cm_rf <- confusionMatrix(preds_class, test_data$Response, positive = "Yes")
```

# ==========================================
# 5. Results and variable importance
# ==========================================
```{r}
print(cm_rf)
cat("\nAUC (Area Under the Curve):", auc(roc_obj))
cat("\nBest Decision Threshold:", best_threshold)

# Variable Importance Plot
# This indicates which variables actually drive the decision
varImpPlot(rf_model$finalModel, main = "Top Influential Variables for Purchase", n.var = 20)
```

# ==========================================
# 6. Representative tree visualization
# ==========================================
```{r}
# Train a simple tree JUST for visualization of business rules.
# We mimic the balancing effect performed by the Random Forest.

tree_viz <- rpart(
  Response ~ ., 
  data = train_data,
  method = "class",
  parms = list(prior = c(0.5, 0.5)),  # Trick to visually balance importance
  control = rpart.control(cp = 0.005, maxdepth = 5) # Limit depth for readability
)

# Print the tree
rpart.plot(tree_viz,
           main = "Business Rules Tree (Buyer Profile)",
           type = 4,             # Draws nice labels
           extra = 104,          # Shows probability for each class (multiclass)
           under = TRUE,         # Places the % under the box
           faclen = 0,           # Shows full variable names
           cex = 0.7,            # Text size
           box.palette = "BuGn"  # Colors: Green for YES, Blue/Grey for NO
)
```
