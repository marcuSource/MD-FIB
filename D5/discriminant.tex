\section{Discriminant Models}

\subsection{Naive Bayes Classifier}
This algorithm applies Bayes' Theorem with a strong assumption of independence between features, allowing us to explicitly observe how probability distributions shape the decision boundary.

\subsubsection{Methodological Approach}
We implemented two distinct variations using the \texttt{naivebayes} library to test different distributional assumptions:

\begin{itemize}
    \item \textbf{Gaussian Naive Bayes:} Assumes that numerical features (like \texttt{Income} or \texttt{WineExp}) follow a strict Normal (Bell curve) distribution.
    \item \textbf{Kernel Density Estimation (KDE):} A non-parametric approach that learns the actual shape of the data distribution, allowing for "weird" or multi-modal shapes that are closer to reality.
\end{itemize}

\subsubsection{Model Performance Analysis}
The models were evaluated on the test set ($N=609$). The results highlight significant limitations in the algorithm's ability to handle this specific transactional dataset.

\subsubsection{Standard Gaussian Failure}
The standard model performed poorly, achieving an accuracy of only 81.44\%.
\begin{itemize}
    \item \textbf{The "No Information" Problem:} The No Information Rate (NIR)—simply guessing "No" for everyone—was 84.73\%. 
    \item \textbf{Result:} Since the model's accuracy (81.44\%) is \textit{lower} than the NIR, the Gaussian model effectively performs worse than a random guess. This confirms that customer spending behavior does \textit{not} follow a normal distribution; forcing this assumption introduced significant error.
\end{itemize}

\subsubsection{Kernel Density Results (KDE)}
By switching to Kernel Density Estimation, the model adapted better to the real data shape, but revealed a critical flaw for our business goal.

\begin{itemize}
    \item \textbf{Accuracy (87.4\%):} 
    \[ \frac{499 + 33}{609} = 0.8736 \]
    The KDE model successfully surpassed the NIR baseline, indicating some predictive power.
    
    \item \textbf{Specificity (96.7\%):}
    The model became extremely precise at rejecting non-buyers, generating only 17 False Positives. It is very "safe."
    
    \item \textbf{Sensitivity (35.5\%) - The Dealbreaker:}
    \[ \frac{33}{33 + 60} = 0.355 \]
    Despite high precision, the model missed nearly **65\%** of the actual buyers (60 False Negatives). For a revenue-focused campaign, missing two-thirds of the target audience is unacceptable.
\end{itemize}

\subsubsection{Decision Landscape Visualization}
Figure \ref{fig:nb_heatmap} illustrates the root cause of the model's struggle compared to tree-based methods.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{Images/heatmapNaive.png}
    \caption{Naive Bayes Decision Landscape: The "Blurry" Boundary}
    \label{fig:nb_heatmap}
\end{figure}

The heatmap reveals a smooth, diffuse probability gradient (the dark blue to yellow zones). Unlike the sharp, rectangular decision boundaries seen in other predictors, Naive Bayes creates "blurry" transitions.

\subsubsection{Diagnostic: The Independence Trap}
The smooth transitions are a symptom of the \textbf{Independence Assumption}. 
\begin{itemize}
    \item The model treats variables like \texttt{TotalExp} (Total Spend) and \texttt{WineExp} (Wine Spend) as completely unrelated independent events.
    \item In reality, our data contains high multicollinearity. Because Naive Bayes cannot capture the interaction effects between correlated variables, it fails to isolate the specific "high-propensity pockets" that the Random Forest found, resulting in a conservative model that defaults to "No" too often.
\end{itemize}

\subsubsection{Conclusion}
While the Kernel Density Naive Bayes improved upon the Gaussian baseline, we recommend \textbf{discarding} it for the final deployment. The gain in Specificity does not justify the massive loss in potential revenue caused by its low Sensitivity (35.5\%).

\subsection{k-Nearest Neighbour}

To investigate whether customer similarity (proximity in feature space) dictates purchasing behavior, we implemented a k-Nearest Neighbors (kNN) classifier. Unlike tree-based methods that slice data using rules, kNN relies on geometric distance (Euclidean) to classify a customer based on their similarity to known buyers.

\subsubsection{Optimization Strategy}
Given the severe class imbalance (only $\sim$15\% buyers), standard accuracy is a misleading metric. Therefore, we optimized the hyperparameter $k$ (number of neighbors), ensuring a balance between Precision and Recall. We tested odd values from $k=1$ to $k=25$.

\subsubsection{Performance Analysis}
The model's performance on the test set ($N=609$) was unsatisfactory, revealing fundamental limitations of distance-based algorithms in high-dimensional marketing data.

Figure \ref{fig:knn_elbow} shows the optimization curve.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Images//knnModel.png}
    \caption{Optimization Curve: F1-Score vs Neighbors ($k$)}
    \label{fig:knn_elbow}
\end{figure}

Paradoxically, the algorithm selected $k=1$ as the optimal value. 
\begin{itemize}
    \item \textbf{Interpretation:} A $k=1$ model implies that the decision boundary is extremely jagged and sensitive to noise. The model is essentially "memorizing" isolated data points rather than learning a generalized pattern.
\end{itemize}

The breakdown of predictions is visualized in Figure \ref{fig:knn_heatmap}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Images/heatmapKNN.png}
    \caption{kNN Confusion Matrix ($k=1$)}
    \label{fig:knn_heatmap}
\end{figure}

The metrics confirm the model's inability to detect buyers reliably:
\begin{itemize}
    \item \textbf{Accuracy (82.4\%):} 
    Lower than the No Information Rate (84.7\%). The model performs worse than a "dummy" classifier that simply predicts "No" for everyone.
    
    \item \textbf{Sensitivity (28.0\%):}
    \[ \frac{26}{26 + 67} = 0.279 \]
    \textbf{Critical Failure:} The model missed 72\% of the target audience (67 False Negatives). In a crowded feature space, the "buyers" are too sparse for the algorithm to form cohesive clusters.
    
    \item \textbf{Specificity (92.2\%):}
    While it correctly identified non-buyers, this is simply a byproduct of the model's tendency to predict the majority class.
\end{itemize}

\subsubsection{Conclusion}
The kNN model suffered from the Curse of Dimensionality. With nearly 20 predictor variables (Income, Spending across 6 categories, Web Visits, etc.), the concept of "distance" becomes diluted, making it impossible for the algorithm to distinguish true buyer clusters. We recommend discarding this model.

\subsection{Linear Discriminant Analysis (LDA)}

Linear Discriminant Analysis (LDA) was implemented as a parametric classification technique to model the binary target variable \textit{Response}. This method assumes that the explanatory variables within each class are normally distributed and share a common covariance matrix. By projecting the multi-dimensional feature space onto a lower-dimensional axis that maximizes the ratio of between-class variance to within-class variance, LDA creates a linear decision boundary. This approach is particularly valuable for this analysis due to its high interpretability, allowing for a clear understanding of the specific financial and behavioral drivers that characterize customers who accept the marketing campaign versus those who do not.

\subsubsection{Methodology and Model Setup}

The implementation of the LDA model required specific preprocessing steps to satisfy the algorithm's statistical assumptions and optimize its performance for the target variable:

\begin{enumerate}
    \item \textbf{Feature Selection:} The analysis utilized a subset of continuous numerical variables. This included financial metrics (e.g., \textit{Income}, \textit{WineExp}, \textit{MeatExp}) and behavioral indicators (e.g., \textit{Recency}, \textit{WebVisits}, \textit{StorePurc}). Categorical variables and identifiers were excluded to maintain a continuous feature space suitable for linear discrimination.
    \item \textbf{Standardization:} All predictor variables were standardized (centered to a mean of 0 and scaled to a standard deviation of 1). This step is mathematically essential in LDA to ensure that the resulting scaling coefficients are directly comparable. Without standardization, variables with larger magnitudes (like \textit{Income}) would artificially dominate the discriminant function compared to variables with smaller ranges (like \textit{NumWebPurchases}).
    \item \textbf{Handling Class Imbalance via Priors:} The dataset exhibits a significant imbalance, with approximately 15\% of customers responding positively ("Yes") and 85\% negatively ("No"). A standard LDA using natural priors would bias the prediction heavily towards the majority class to maximize global accuracy, often resulting in a Sensitivity near zero. To address this and align with the business objective of identifying potential buyers, we enforced \textbf{equal prior probabilities} (\texttt{prior = c(0.5, 0.5)}). This adjustment compels the algorithm to weigh the minority class ("Yes") equally, thereby prioritizing the detection of true positives.
\end{enumerate}

\subsubsection{Visual Analysis of Separation}

The core mechanic of LDA is the creation of a Linear Discriminant axis (LD1). Figure \ref{fig:lda_density} illustrates the distribution of the transformed data along this new axis.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{Images/LDA_1_Density_Separation.png}
    \caption{Density Plot of Linear Discriminant Scores (LD1)}
    \label{fig:lda_density}
\end{figure}

The density plot reveals a distinct separation between the two classes. The distribution of customers who accepted the offer (Teal) is shifted significantly to the right compared to those who declined (Red). While there is an area of overlap, which represents the inherent difficulty in separating marginal cases, the distinct peaks indicate that the linear combination of the selected features contains strong discriminative power regarding the customer response.

To visualize this separation in the context of the original variables, Figure \ref{fig:lda_ellipses} plots the data based on the two most significant features, overlaid with 95\% confidence ellipses.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{Images/LDA_5_Ellipses.png}
    \caption{Confidence Ellipses: Income vs. Wine Expenditure}
    \label{fig:lda_ellipses}
\end{figure}

The confidence ellipses demonstrate a clear divergence in the geometric centers of the two groups. The "Yes" class is clustered in the upper-right quadrant, corresponding to higher income levels and higher expenditure on wine products, whereas the "No" class is concentrated in the lower-left region.

\subsubsection{Model Interpretation: Key Drivers}

A key advantage of LDA is the ability to interpret the scaling coefficients to identify the specific drivers of the classification decision. Figure \ref{fig:lda_importance} displays the variables with the highest absolute influence on the discriminant function.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{Images/LDA_2_Feature_Importance.png}
    \caption{Top Determinant Factors (LDA Coefficients)}
    \label{fig:lda_importance}
\end{figure}

The analysis of the coefficients highlights the following behavioral patterns:
\begin{itemize}
    \item \textbf{WineExp (Strong Positive):} This variable is the dominant predictor. A high positive coefficient indicates that expenditure on wine is the single most significant characteristic distinguishing responders from non-responders.
    \item \textbf{Recency (Strong Negative):} The \textit{Recency} variable has a substantial negative weight. Since this metric measures the days elapsed since the last purchase, a negative coefficient implies that lower values (i.e., more recent activity) increase the likelihood of a positive response.
    \item \textbf{MeatExp and WebVisits (Positive):} Secondary drivers include expenditure on meat products and the frequency of web visits, suggesting that the target demographic is not only affluent but also digitally engaged.
\end{itemize}

\subsubsection{Performance Evaluation}

The model's predictive capability was assessed using a held-out test set comprising 20\% of the data. The confusion matrix in Table \ref{tab:lda_confusion} details the classification results.

\begin{table}[H]
\centering
\caption{Confusion Matrix (LDA - Test Set)}
\label{tab:lda_confusion}
\begin{tabular}{@{}llcc@{}}
\toprule
& & \multicolumn{2}{c}{\textbf{Actual Class}} \\
& & No (0) & Yes (1) \\ \midrule
\textbf{Predicted} & No (0) & 281 & 17 \\
\textbf{Class} & Yes (1) & 61 & 48 \\ \bottomrule
\end{tabular}
\end{table}

The performance metrics derived from this matrix are as follows:
\begin{itemize}
    \item \textbf{Accuracy (80.84\%):} The model correctly classified over 80\% of the cases. While this provides a general baseline, it is secondary to metrics that specifically evaluate the minority class in this marketing context.
    \item \textbf{Sensitivity / Recall (73.85\%):} This is the critical metric for the campaign's success. The model successfully identified approximately 74\% (48 out of 65) of the actual positive responders. This high sensitivity confirms that the adjustment of prior probabilities was effective in capturing the target audience.
    \item \textbf{Specificity (82.16\%):} The model maintains a robust ability to correctly identify non-responders, ensuring that marketing resources are not wasted on customers unlikely to convert.
    \item \textbf{AUC (0.8278):} The Area Under the Curve is well above 0.8, indicating strong separability and a reliable ranking of probabilities.
\end{itemize}

The Receiver Operating Characteristic (ROC) curve in Figure \ref{fig:lda_roc} visually confirms the model's performance, showing a trajectory that rises steeply toward the top-left corner, indicative of a good trade-off between the True Positive Rate and False Positive Rate.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{Images/LDA_4_ROC_Curve.png}
    \caption{ROC Curve for LDA (AUC = 0.828)}
    \label{fig:lda_roc}
\end{figure}

\subsubsection{Decision Boundaries}

Finally, Figure \ref{fig:lda_boundary} illustrates the decision boundary generated by the model in the two-dimensional space defined by the primary predictors, \textit{Income} and \textit{WineExp}.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{Images/LDA_3_Decision_Boundaries.png}
    \caption{LDA Linear Decision Boundaries (Income vs Wine Expenditure)}
    \label{fig:lda_boundary}
\end{figure}

The partition plot demonstrates the linearity of the separation. The LDA algorithm divides the feature space with a straight line, classifying the upper-right region—characterized by high income and high wine expenditure—as the "Yes" response zone (Gold), and the lower-left region as the "No" response zone (Light Blue). This clear, linear demarcation underscores the model's reliance on the strong correlation between purchasing power, product affinity, and campaign acceptance.

\subsubsection{Conclusion on LDA}

The Linear Discriminant Analysis provided a robust framework for modeling the customer \textit{Response}. By enforcing equal priors, the model overcame the inherent class imbalance to achieve a Sensitivity of \textbf{73.85\%}, effectively capturing the majority of potential buyers. The analysis of the discriminant coefficients offers a clear business profile for the ideal target: the campaign is most likely to succeed with customers who have a high affinity for wine products and have engaged with the store recently (low Recency), regardless of the channel used.
