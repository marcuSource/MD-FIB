---
title: "Naive iFood"
author: "iFood Marketing Analytics"
date: "`r format(Sys.Date(), '%d %B %Y')`"
output:
  html_document:
  toc: true
toc_float: true
toc_depth: 3
theme: united
highlight: tango
code_folding: show
pdf_document: default
---
  
# Global knitr configuration for the R Markdown document:
```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = TRUE,
  fig.width = 10,
  fig.height = 7
)
```
# This script implements and evaluates the Naive Bayes classifier 
# to predict the Response variable.

```{r}
library(readr)
library(dplyr)
library(naivebayes) # For the Naive Bayes algorithm
library(caret)      # For the createDataPartition and confusionMatrix functions
```

#1.Setup and data preparation
```{r}
# Load dataset
data <- read_csv("ifood_enriched.csv", show_col_types = FALSE)

# Column name cleaning
names(data) <- make.names(names(data))

# Impute missing values (NA) in 'Income' with the median
data$Income[is.na(data$Income)] <- median(data$Income, na.rm = TRUE)

# Variable Selection
# We remove 'CustomerSegment' to focus purely on the binary Response
dd_model <- data %>%
  select(
    Response, 
    Income, Age, Recency,
    WineExp, MeatExp, FishExp, SweetExp, GoldExp, TotalExp,
    TotalPurchases, DealsPurc, WebPurc, CatalogPurc, StorePurc,
    Kidhome, Teenhome, Education, MaritalSts, 
    WebVisits, AccCmp3, AccCmp4, AccCmp5, AccCmp1, AccCmp2, Complain,
    EngagementIndex, PropensityScore
  ) %>%
  na.omit()

# CRITICAL STEP: Convert target to Factor with readable labels
# 0 -> "No", 1 -> "Yes"
dd_model$Response <- factor(dd_model$Response, 
                            levels = c(0, 1), 
                            labels = c("No", "Yes"))
```

#2.Data Splitting
```{r}
set.seed(42) # For reproducibility

# Create stratified partition (70% Train, 30% Test)
# We use p=0.7 to generate the Training Index
train_index <- createDataPartition(dd_model$Response, p = 0.7, list = FALSE)

dataTrain <- dd_model[train_index,]
dataTest  <- dd_model[-train_index,]

cat("Training Set:", nrow(dataTrain), "observations.\n")
cat("Test Set:", nrow(dataTest), "observations.\n")
```

#3.Standard Naive Bayes Model
```{r}
# Train the Naive Bayes model
# Assumes normal distribution (Bell curve) for numerical variables
nb_standard <- naive_bayes(Response ~ ., data = dataTrain, laplace = 1)

# Prediction on the test set
pred_standard <- predict(nb_standard, dataTest)

# Evaluation: Confusion Matrix
cat("\n--- Confusion Matrix (Standard Gaussian Model) ---\n")
# We specify positive="Yes" to ensure Sensitivity is calculated for buyers
conf_matrix_std <- confusionMatrix(pred_standard, dataTest$Response, positive = "Yes")
print(conf_matrix_std)
```

#4.Naive Bayes with Kernel Density Estimation
```{r}
# Train the Kernel model (usekernel = TRUE)
# This allows the model to fit "weird" shapes of data distributions, 
# not just bell curves. It is usually more accurate for real-world data.

nb_kernel <- naive_bayes(x = dataTrain %>% select(-Response), 
                         y = dataTrain$Response, 
                         usekernel = TRUE)

# Prediction on the test set
pred_kernel <- predict(nb_kernel, dataTest)

# Evaluation: Confusion Matrix
cat("\n--- Confusion Matrix (Kernel Density Model) ---\n")
conf_matrix_ker <- confusionMatrix(pred_kernel, dataTest$Response, positive = "Yes")
print(conf_matrix_ker)
```

#5.Visualization
```{r}
# Naive Bayes works by comparing probability distributions.
# Let's visualize how the model distinguishes Buyers vs Non-Buyers 
# based on the most important variable: EngagementIndex.

# We use the built-in plot method for naive_bayes objects
# It shows the "Density" of Yes (Blue) vs No (Red)

cat("\nVisualizing how the model separates customers based on Engagement:\n")
plot(nb_kernel, legend.box = TRUE, which = "EngagementIndex", arg.num = list(col = c("red", "blue"), main = "Engagement Index: Buyers (Blue) vs Non-Buyers (Red)"))

cat("\nVisualizing how the model separates customers based on Total Expenses:\n")
plot(nb_kernel, legend.box = TRUE, which = "TotalExp", arg.num = list(col = c("red", "blue"), main = "Total Spend: Buyers (Blue) vs Non-Buyers (Red)"))
```

#Final Conclusions
```{r}
# --- PERFORMANCE SUMMARY ---
# 1. The Gaussian Naive Bayes model showed a low Accuracy (81.44%), performing worse 
#    than the No Information Rate (84.73%). This indicates that assuming a normal 
#    distribution for customer spending behavior is incorrect.
#
# 2. The Kernel Density Estimation (KDE) significantly improved Precision (from 41% to 66%) 
#    and Accuracy (87.36%), confirming that the data follows complex, non-normal distributions.
#    However, this improvement came at a high cost: Sensitivity dropped to 35.5%.

# --- COMPARATIVE DIAGNOSIS (Naive Bayes vs. Random Forest) ---
# The Naive Bayes classifier failed to compete with the previously trained Random Forest 
# (which achieved ~89% Sensitivity). 
#
# The root cause is the "Independence Assumption". Naive Bayes assumes that variables 
# like 'TotalExp' (Total Spend) and 'MeatExp' (Meat Spend) are unrelated. 
# In reality, our iFood dataset contains highly correlated variables (multicollinearity).
#
# While Random Forest exploits these correlations to boost performance, Naive Bayes 
# "double counts" the evidence, leading to calibrated probabilities that are either 
# too aggressive (Gaussian) or too conservative (Kernel).
```
