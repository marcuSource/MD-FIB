---
title: "Naive iFood"
author: "iFood Marketing Analytics"
date: "`r format(Sys.Date(), '%d %B %Y')`"
output:
  html_document:
  toc: true
toc_float: true
toc_depth: 3
theme: united
highlight: tango
code_folding: show
pdf_document: default
---
  
# Global knitr configuration for the R Markdown document:
```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = TRUE,
  fig.width = 10,
  fig.height = 7
)
```
# This script implements and evaluates the Naive Bayes classifier 
# to predict the Response variable.

```{r}
library(readr)
library(dplyr)
library(naivebayes) # For the Naive Bayes algorithm
library(caret)      # For the createDataPartition and confusionMatrix functions
```

#1.Setup and data preparation
```{r}
# Load dataset
data <- read_csv("ifood_enriched.csv", show_col_types = FALSE)

# Column name cleaning
names(data) <- make.names(names(data))

# Impute missing values (NA) in 'Income' with the median
data$Income[is.na(data$Income)] <- median(data$Income, na.rm = TRUE)

# Variable Selection
# We remove 'CustomerSegment' to focus purely on the binary Response
dd_model <- data %>%
  select(
    Response, 
    Income, Age, Recency,
    WineExp, MeatExp, FishExp, SweetExp, GoldExp, TotalExp,
    TotalPurchases, DealsPurc, WebPurc, CatalogPurc, StorePurc,
    Kidhome, Teenhome, Education, MaritalSts, 
    WebVisits, AccCmp3, AccCmp4, AccCmp5, AccCmp1, AccCmp2, Complain,
    EngagementIndex, PropensityScore
  ) %>%
  na.omit()

# CRITICAL STEP: Convert target to Factor with readable labels
# 0 -> "No", 1 -> "Yes"
dd_model$Response <- factor(dd_model$Response, 
                            levels = c(0, 1), 
                            labels = c("No", "Yes"))
```

#2.Data Splitting
```{r}
set.seed(42) # For reproducibility

# Create stratified partition (70% Train, 30% Test)
# We use p=0.7 to generate the Training Index
train_index <- createDataPartition(dd_model$Response, p = 0.7, list = FALSE)

dataTrain <- dd_model[train_index,]
dataTest  <- dd_model[-train_index,]

cat("Training Set:", nrow(dataTrain), "observations.\n")
cat("Test Set:", nrow(dataTest), "observations.\n")
```

#3.Standard Naive Bayes Model
```{r}
# Train the Naive Bayes model
# Assumes normal distribution (Bell curve) for numerical variables
nb_standard <- naive_bayes(Response ~ ., data = dataTrain, laplace = 1)

# Prediction on the test set
pred_standard <- predict(nb_standard, dataTest)

# Evaluation: Confusion Matrix
cat("\n--- Confusion Matrix (Standard Gaussian Model) ---\n")
# We specify positive="Yes" to ensure Sensitivity is calculated for buyers
conf_matrix_std <- confusionMatrix(pred_standard, dataTest$Response, positive = "Yes")
print(conf_matrix_std)
```

#4.Naive Bayes with Kernel Density Estimation
```{r}
# Train the Kernel model (usekernel = TRUE)
# This allows the model to fit "weird" shapes of data distributions, 
# not just bell curves. It is usually more accurate for real-world data.

nb_kernel <- naive_bayes(x = dataTrain %>% select(-Response), 
                         y = dataTrain$Response, 
                         usekernel = TRUE)

# Prediction on the test set
pred_kernel <- predict(nb_kernel, dataTest)

# Evaluation: Confusion Matrix
cat("\n--- Confusion Matrix (Kernel Density Model) ---\n")
conf_matrix_ker <- confusionMatrix(pred_kernel, dataTest$Response, positive = "Yes")
print(conf_matrix_ker)
```

# ==========================================
# 5.Naive Bayes Heatmap
# ==========================================
```{r, fig.width=10, fig.height=8}
library(ggplot2)
library(viridis)

# 1. Select the same variables as the Random Forest for fair comparison
var1 <- "EngagementIndex"
var2 <- "PropensityScore"

# 2. Create the Grid
grid_size <- 100
x_range <- seq(min(dataTrain[[var1]]), max(dataTrain[[var1]]), length.out = grid_size)
y_range <- seq(min(dataTrain[[var2]]), max(dataTrain[[var2]]), length.out = grid_size)
grid_data <- expand.grid(x = x_range, y = y_range)
names(grid_data) <- c(var1, var2)

# 3. Fill the rest of the variables (Robust Fill)
# Naive Bayes is very sensitive, so we fill with Medians/Modes to neutralize other factors
other_cols <- setdiff(names(dataTrain), c(var1, var2, "Response"))

for(col in other_cols){
  if(col %in% names(dataTrain)){
    if(is.numeric(dataTrain[[col]])){
      grid_data[[col]] <- median(dataTrain[[col]], na.rm=TRUE)
    } else {
      # For factors/text, take the first level/value
      first_val <- na.omit(unique(dataTrain[[col]]))[1]
      grid_data[[col]] <- first_val
    }
  }
}

# 4. PREDICTION (Specific for Naive Bayes)
# The predict function for naive_bayes returns a matrix. We want the "Yes" column.
probs_nb <- predict(nb_kernel, newdata = grid_data, type = "prob")

# Robustly select the probability for "Yes"
# Check if "Yes" is a column name, otherwise take the 2nd column
if("Yes" %in% colnames(probs_nb)){
  grid_data$Probability <- probs_nb[, "Yes"]
} else {
  grid_data$Probability <- probs_nb[, 2]
}

# 5. PLOT
ggplot(grid_data, aes_string(x = var1, y = var2, fill = "Probability")) +
  geom_tile() + 
  scale_fill_viridis(option = "cividis", direction = 1, name = "Prob. of Yes (NB)") +
  
  # Overlay REAL test points
  geom_point(data = dataTest, aes_string(x = var1, y = var2, color = "Response"), 
             alpha = 0.6, size = 1.5, inherit.aes = FALSE) +
  scale_color_manual(values = c("black", "cyan"), 
                     labels = c("Did Not Buy", "Bought"), 
                     name = "Actual Outcome") +
  
  theme_minimal(base_size = 14) +
  labs(
    title = "Naive Bayes Decision Landscape: The 'Blurry' Boundary",
    subtitle = "Notice how the probability zones are smoother and less defined than Random Forest",
    x = "Engagement Index",
    y = "Propensity Score"
  ) +
  theme(legend.position = "right")
```

#Final Conclusions
```{r}
# --- PERFORMANCE SUMMARY ---
# 1. The Gaussian Naive Bayes model showed a low Accuracy (81.44%), performing worse 
#    than the No Information Rate (84.73%). This indicates that assuming a normal 
#    distribution for customer spending behavior is incorrect.
#
# 2. The Kernel Density Estimation (KDE) significantly improved Precision (from 41% to 66%) 
#    and Accuracy (87.36%), confirming that the data follows complex, non-normal distributions.
#    However, this improvement came at a high cost: Sensitivity dropped to 35.5%.

# --- COMPARATIVE DIAGNOSIS (Naive Bayes vs. Random Forest) ---
# The Naive Bayes classifier failed to compete with the previously trained Random Forest 
# (which achieved ~89% Sensitivity). 
#
# The root cause is the "Independence Assumption". Naive Bayes assumes that variables 
# like 'TotalExp' (Total Spend) and 'MeatExp' (Meat Spend) are unrelated. 
# In reality, our iFood dataset contains highly correlated variables (multicollinearity).
#
#While Random Forest exploits these correlations to boost performance, Naive Bayes 
# treats them separately, leading to less precise decision boundaries (as seen in the 
# smooth heatmap above).
```
