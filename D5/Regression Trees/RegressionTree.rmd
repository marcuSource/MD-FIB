---
title: "Regression Trees iFood"
author: "iFood Marketing Analytics"
date: "`r format(Sys.Date(), '%d %B %Y')`"
output:
  html_document:
  toc: true
toc_float: true
toc_depth: 3
theme: united
highlight: tango
code_folding: show
pdf_document: default
---

# ==========================================
# 1. Setup and Data Loading
# ==========================================
```{r setup, message=FALSE, warning=FALSE}
library(readr)
library(dplyr)
library(caret)
library(rpart)
library(rpart.plot)

# Load dataset
data <- read_csv("ifood_enriched.csv", show_col_types = FALSE)
names(data) <- make.names(names(data))

# Quick Imputation
data$Income[is.na(data$Income)] <- median(data$Income, na.rm = TRUE)

# Variable Selection
df_tree <- data %>%
  select(
    Response, Income, Age, Recency,
    WineExp, MeatExp, FishExp, SweetExp, FruitExp, GoldExp, TotalExp,
    TotalPurchases, DealsPurc, WebPurc, CatalogPurc, StorePurc,
    Kidhome, Teenhome, Education, MaritalSts, 
    WebVisits, AccCmp3, AccCmp4, AccCmp5, AccCmp1, AccCmp2, Complain,
    CustomerTenure, CampaignAcceptanceRate, PropensityScore, EngagementIndex
  ) %>%
  na.omit()
```

# ==========================================
# 2. Data Conversion (Binary to Numeric)
# ==========================================
```{r}
# To perform a REGRESSION tree on a binary response,
# we must first convert "Yes"/"No" (or factor 1/0) into a pure numeric 1/0.

# If your Response variable is already numeric 0/1 in the CSV:
df_tree$ResponseNum <- as.numeric(df_tree$Response)

# If your variable loaded as Factor or Text, ensure it becomes 0 or 1:
# (This line is a safety check; if it's already numeric, it won't hurt)
if(is.factor(df_tree$ResponseNum)) {
  df_tree$ResponseNum <- as.numeric(as.character(df_tree$Response)) 
}

# Remove the original 'Response' variable to avoid confusing the model
df_model <- df_tree %>% select(-Response)
```

# ==========================================
# 3. Model Training
# ==========================================
```{r}
# method = "anova" tells R: "Treat this as a mathematical regression".
# This calculates the AVERAGE of 0s and 1s (which equals the probability).
reg_tree <- rpart(
  ResponseNum ~ ., 
  data = df_model,
  method = "anova",  
  control = rpart.control(cp = 0.005) # Low cp to allow the tree to grow slightly more
)
```

# ==========================================
# 4. Visualization (Probability Map)
# ==========================================
```{r}
rpart.plot(reg_tree,
           main = "Purchase Probability Tree (0 = 0%, 1 = 100%)",
           type = 4,
           extra = 101,  # Shows the mean value (probability) and n (count)
           digits = 3,   # 3 decimal places for clarity
           box.palette = "RdYlGn", # Red (Low prob) -> Green (High prob)
           tweak = 1.2   # Adjust text size
)
```

# ==========================================
# 5. Decision Heatmap
# ==========================================
```{r, fig.width=10, fig.height=8}
library(ggplot2)
library(viridis) # Professional color palettes

# 1. Identify the top 2 most powerful variables (from the previous varImpPlot)
# Adjust these names if your model indicates other top drivers
var1 <- "EngagementIndex"
var2 <- "PropensityScore"

# 2. Create a synthetic 'Grid'
# This simulates thousands of imaginary customers to map the model's decision surface
grid_size <- 100
x_range <- seq(min(df_model[[var1]]), max(df_model[[var1]]), length.out = grid_size)
y_range <- seq(min(df_model[[var2]]), max(df_model[[var2]]), length.out = grid_size)
grid_data <- expand.grid(x = x_range, y = y_range)
names(grid_data) <- c(var1, var2)

# 3. Fill the rest of the variables with the median
# The model requires ALL columns to make a prediction, so we neutralize the others
other_cols <- setdiff(names(df_model), c(var1, var2, "ResponseNum"))
for(col in other_cols){
  grid_data[[col]] <- median(df_model[[col]], na.rm=TRUE)
  # If it's a factor, use the most common level
  if(is.factor(df_model[[col]])) {
    grid_data[[col]] <- levels(df_model[[col]])[1] 
  }
}

# 4. Predict probability for the entire grid
grid_data$Probability <- predict(reg_tree, newdata = grid_data)

# 5. THE PLOT (The 'Treasure Map')
ggplot(grid_data, aes_string(x = var1, y = var2, fill = "Probability")) +
  geom_tile() + # Draws the heatmap tiles
  scale_fill_viridis(option = "magma", direction = -1, name = "Purch. Prob.") +
  
  # Overlay REAL customers as points
  # Cyan points = Bought (Response ~ 1), Black points = Did not buy
  geom_point(data = df_model, aes_string(x = var1, y = var2, color = "as.factor(ResponseNum)"), 
             alpha = 0.4, size = 1.5, inherit.aes = FALSE) +
  scale_color_manual(values = c("black", "cyan"), 
                     labels = c("No Purchase", "Purchase"), 
                     name = "Actual Status") +
  
  theme_minimal(base_size = 14) +
  labs(
    title = "Decision Boundary Heatmap: The 'Sweet Spot'",
    subtitle = "Visualizing high-probability zones for Campaign Acceptance",
    x = "Engagement Index",
    y = "Propensity Score"
  ) +
  theme(legend.position = "right")
```
